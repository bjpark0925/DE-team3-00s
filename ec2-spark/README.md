# ec2-spark

## ğŸ“Œ ê°œìš”

EC2 ì¸ìŠ¤í„´ìŠ¤ì—ì„œ Docker ê¸°ë°˜ Spark í´ëŸ¬ìŠ¤í„°ë¥¼ ìë™ìœ¼ë¡œ êµ¬ì„±í•˜ê³ ,

NYC Taxi ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¥¼ PostgreSQLì— ì €ì¥í•˜ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„

`app.py`ëŠ” Spark ì‘ì—… ìˆ˜í–‰, `db.py`ëŠ” PostgreSQL í…Œì´ë¸” ìµœì í™”ë¥¼ ë‹´ë‹¹

---

## ì‹¤í–‰ í™˜ê²½

- **Docker / ubuntu:22.04**
- **Apache Spark 3.5.6, PySpark 3.5.6**
- **PostgreSQL**

---

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
ec2-spark
â”œâ”€â”€ app.py                  # ë©”ì¸ Spark ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ db.py                   # PostgreSQL í…Œì´ë¸” í›„ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
```

---

## ğŸ” ì „ì²´ ì•„í‚¤í…ì²˜ íë¦„

1. EC2 ì¸ìŠ¤í„´ìŠ¤ê°€ ì‹œì‘ë˜ë©´ `user-data.sh`ë¥¼ í†µí•´ ë‹¤ìŒ ì‘ì—…ì´ ìë™ ìˆ˜í–‰ë©ë‹ˆë‹¤:
   - í•„ìˆ˜ íŒ¨í‚¤ì§€ ë° Docker ì„¤ì¹˜
   - ECRì—ì„œ Spark ì´ë¯¸ì§€ pull
   - `docker-compose.yml` ìë™ ìƒì„± ë° Spark í´ëŸ¬ìŠ¤í„° ì‹¤í–‰

2. Docker Composeë¥¼ í†µí•´ ë‹¤ìŒ ì»¨í…Œì´ë„ˆë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:
   - `spark-master`: Spark ë§ˆìŠ¤í„° ë…¸ë“œ
   - `spark-worker1`, `spark-worker2`: ì›Œì»¤ ë…¸ë“œ 2ê°œ

3. Spark ë§ˆìŠ¤í„°ê°€ ì •ìƒ ì‘ë™í•˜ë©´, ë‹¤ìŒ ë‹¨ê³„ì˜ ETL íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ë©ë‹ˆë‹¤:
   - `spark-submit`ìœ¼ë¡œ `app.py` ì œì¶œ
   - **Extract**: S3ì—ì„œ NYC íƒì‹œ ë°ì´í„° ë¡œë“œ
   - **Transform**:
     - ìœ íš¨ì„± ê²€ì‚¬ ë° ì „ì²˜ë¦¬ (ì‹œê°„ ì¡°ê±´, ê¸ˆì•¡ í•„í„° ë“±)
     - ì˜ˆìƒ í”½ì—…ëŸ‰(`expected_pickups`), ì¸ì ‘ ì§€ì—­(`pick_dropoff_zones`) ê³„ì‚°
   - **Load**:
     - ë‘ ê°œì˜ ê²°ê³¼ í…Œì´ë¸”ì„ PostgreSQLë¡œ ì €ì¥

4. Spark ì‘ì—… ì¢…ë£Œ í›„ `db.py`ë¥¼ ì‹¤í–‰í•˜ì—¬ ë‹¤ìŒ ì‘ì—… ìˆ˜í–‰:
   - ì¤‘ë³µëœ ë°ì´í„°ë¥¼ ì •ë¦¬í•˜ê³  ìµœì¢… `adjacent_zones` í…Œì´ë¸” ìƒì„±
   - ë¶ˆí•„ìš”í•œ ì¤‘ê°„ í…Œì´ë¸” ì œê±°

ì´ íë¦„ì€ ì™„ì „ **ìë™í™”**ë˜ì–´ ìˆìœ¼ë©°, EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ê³¼ ë™ì‹œì— Spark ê¸°ë°˜ ETL íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ë˜ê³  PostgreSQLì— ë°ì´í„°ê°€ ì ì¬ë©ë‹ˆë‹¤.

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

1. **EC2 Launch Templateì„ í†µí•œ ec2 ìƒì„±**

2. **user-data.sh ì£¼ìš” ì‘ì—…**
    - `docker`, `awscli`, `docker-compose` ì„¤ì¹˜
    - ECRì—ì„œ ì´ë¯¸ì§€ pull
    - `docker-compose.yml` ìë™ ìƒì„± ë° ì‹¤í–‰
    - `spark-submit app.py` ì‹¤í–‰
    - ì´í›„ `db.py`ë¥¼ ì‹¤í–‰í•˜ì—¬ PostgreSQL í›„ì²˜ë¦¬

3. **Docker Compose êµ¬ì„± (3ë…¸ë“œ)**
    - `spark-master`
    - `spark-worker1` CPU cores: 2, Memory : 2G
    - `spark-worker2` CPU cores: 2, Memory : 2G

4. **Spark ì œì¶œ ì˜ˆì‹œ**
    ```bash
    docker exec -it spark-master spark-submit       --master spark://master:7077       --deploy-mode client       ...
      app.py
    ```

---

## ğŸ“Š app.py ì£¼ìš” ê¸°ëŠ¥

- **ë°ì´í„° ë¡œë”©**: S3 (NYC Taxi Data)
- **í•„í„°ë§/ì „ì²˜ë¦¬**: ì´ì•¡, ì‹œê°„ ì¡°ê±´, zone ID ìœ íš¨ì„± í•„í„°
- **`expected_pickups` ìƒì„±**:
  - (pickup_zone_id, pickup_hour) ê¸°ì¤€ ì‹œê°„ë‹¹ í‰ê·  í”½ì—… ìˆ˜
- **`pick_dropoff_zones` ìƒì„±**:
  - í‰ê·  ì´ë™ì‹œê°„ 5ë¶„ ë¯¸ë§Œì¸ dropoff_zone_idë§Œ ì¶”ì¶œ
- **PostgreSQL ì €ì¥**

---

## ğŸ§© db.py ì£¼ìš” ê¸°ëŠ¥

- `pick_dropoff_zones`ë¥¼ ê¸°ì¤€ìœ¼ë¡œ `adjacent_zones` ìƒì„±
  - `ARRAY_AGG(DISTINCT dropoff_zone_id)` ì‚¬ìš©
- ë¶ˆí•„ìš” í…Œì´ë¸” ì •ë¦¬ (`DROP TABLE IF EXISTS`)
- ìµœì¢…ì ìœ¼ë¡œ `adjacent_zones`ë§Œ ë‚¨ìŒ

---

## âš™ï¸ í™˜ê²½ ì„¤ì • íŒŒì¼

- `spark-env.sh`:
    - ë§ˆìŠ¤í„° ì£¼ì†Œ, í¬íŠ¸, ì›Œì»¤ ì„¤ì •, ë¡œê·¸ ë””ë ‰í† ë¦¬ ì •ì˜
    - `PYSPARK_PYTHON=python3` ì§€ì •

- `dockerfile`: Spark ë° Python ì‹¤í–‰ í™˜ê²½ í¬í•¨ (ë³„ë„ ë¶„ì„ ìƒëµ ê°€ëŠ¥)

- `user-data.sh`: ì „ì²´ EC2 ì´ˆê¸° ë¶€íŒ… ìë™í™”
    - Docker ì„¤ì¹˜ â†’ ECR ë¡œê·¸ì¸ â†’ docker-compose ìƒì„± ë° ì‹¤í–‰ â†’ spark-submit â†’ db.py ì‹¤í–‰

---


## ğŸ“ ì£¼ì˜ ì‚¬í•­

- `.env` íŒŒì¼ì€ ì»¨í…Œì´ë„ˆì— ë°˜ë“œì‹œ ê³µìœ ë˜ì–´ì•¼ í•¨ (`docker volume` ë˜ëŠ” `ENV`)
- ECR ë¡œê·¸ì¸ ì‹œ region, ê³„ì • ë²ˆí˜¸ ì •í™•íˆ ì…ë ¥í•  ê²ƒ
- PostgreSQLì—ëŠ” í•´ë‹¹ ìŠ¤í‚¤ë§ˆ ê¶Œí•œ í•„ìš”

---

## ğŸ‘¤ ì‘ì„±ì

- GitHub: [your_username]
- ë¬¸ì˜: your_email@example.com

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

1. **EC2 ì¸ìŠ¤í„´ìŠ¤ì— user-data ë“±ë¡ í›„ ë¶€íŒ…**
    ```bash
    # EC2 Launch Templateì— user-data.sh ë“±ë¡
    ```

2. **user-data.sh ì£¼ìš” ì‘ì—…**
    - `docker`, `awscli`, `docker-compose` ì„¤ì¹˜
    - ECRì—ì„œ ì´ë¯¸ì§€ pull
    - `docker-compose.yml` ìë™ ìƒì„± ë° ì‹¤í–‰
    - `spark-submit app.py` ì‹¤í–‰
    - ì´í›„ `db.py`ë¥¼ ì‹¤í–‰í•˜ì—¬ PostgreSQL í›„ì²˜ë¦¬

3. **Docker Compose êµ¬ì„± (3ë…¸ë“œ)**
    - `spark-master` (7077, 8080, 4040, 8888 í¬íŠ¸ ì˜¤í”ˆ)
    - `spark-worker1` (8081)
    - `spark-worker2` (8082â†’8081)

4. **Spark ì œì¶œ ì˜ˆì‹œ**
    ```bash
    docker exec -it spark-master spark-submit       --master spark://master:7077       --deploy-mode client       ...
      ./app.py
    ```

---

## ğŸ“Š app.py ì£¼ìš” ê¸°ëŠ¥

- **ë°ì´í„° ë¡œë”©**: S3 (`s3a://nyc-tlc-softeer/nyc_taxi/`)
- **í•„í„°ë§/ì „ì²˜ë¦¬**: ì´ì•¡, ì‹œê°„ ì¡°ê±´, zone ID ìœ íš¨ì„± í•„í„°
- **`expected_pickups` ìƒì„±**:
  - (pickup_zone_id, pickup_hour) ê¸°ì¤€ ì‹œê°„ë‹¹ í‰ê·  í”½ì—… ìˆ˜
- **`pick_dropoff_zones` ìƒì„±**:
  - í‰ê·  ì´ë™ì‹œê°„ 5ë¶„ ë¯¸ë§Œì¸ dropoff_zone_idë§Œ ì¶”ì¶œ
- **PostgreSQL ì €ì¥**

---

## ğŸ§© db.py ì£¼ìš” ê¸°ëŠ¥

- `pick_dropoff_zones`ë¥¼ ê¸°ì¤€ìœ¼ë¡œ `adjacent_zones` ìƒì„±
  - `ARRAY_AGG(DISTINCT dropoff_zone_id)` ì‚¬ìš©
- ë¶ˆí•„ìš” í…Œì´ë¸” ì •ë¦¬ (`DROP TABLE IF EXISTS`)
- ìµœì¢…ì ìœ¼ë¡œ `adjacent_zones`ë§Œ ë‚¨ìŒ

---

## âš™ï¸ í™˜ê²½ ì„¤ì • íŒŒì¼

- `spark-env.sh`:
    - ë§ˆìŠ¤í„° ì£¼ì†Œ, í¬íŠ¸, ì›Œì»¤ ì„¤ì •, ë¡œê·¸ ë””ë ‰í† ë¦¬ ì •ì˜
    - `PYSPARK_PYTHON=python3` ì§€ì •

- `dockerfile`: Spark ë° Python ì‹¤í–‰ í™˜ê²½ í¬í•¨ (ë³„ë„ ë¶„ì„ ìƒëµ ê°€ëŠ¥)

- `user-data.sh`: ì „ì²´ EC2 ì´ˆê¸° ë¶€íŒ… ìë™í™”
    - Docker ì„¤ì¹˜ â†’ ECR ë¡œê·¸ì¸ â†’ docker-compose ìƒì„± ë° ì‹¤í–‰ â†’ spark-submit â†’ db.py ì‹¤í–‰

---
